# 机器学习——监督学习

> 机器学习的目标是从原始数据中提取特征，学习一个映射函数 f 将上述特征（或原始数据）映射到语义空间，寻找数据和任务目标之间的关系。
>
> - 监督学习
>     - 给定带有标签信息的训练集合，学习从输入到输出的映射
>     - 一般被应用在回归或分类的任务中
> - 无监督学习
>     - 最大特点是**数据无标签**
>     - 一般被应用在聚类或若干降维任务中
>     - 半监督学习依赖于部分被标注的数据
> - 强化学习
>     - 一种序列数据决策学习方法
>     - 从与环境交互中学习，通过回报值（reward）让智能体（agent）学习到在不同状态（state）下如何选择行为方式（action）

监督学习一般包含三个部分内容：

- 从训练数据集中学习得到映射函数 f
- 在测试数据集上测试映射函数 f
- 在未知数据集上测试映射函数 f（投入使用）

训练及中产生的损失一般称为经验风险（empirical risk），越小对训练集拟合效果越好；

测试集中加入从真实数据分布采样的样本时，测试集上的损失会不断逼近期望风险（expected risk），越小模型越好；

机器学习的目标是追求期望风险最小化。

## 损失函数

训练集中一共有 $n$ 个标注数据，第 $i$ 个标注数据记为 $(x_i, y_i)$，其中第 $i$ 个样本数据为 $x_i$，$y_i$ 是 $x_i$ 的标注信息。

从训练数据中学习得到的映射函数记为 $f$，$f$ 对 $x_i$ 的预测结果记为 $f(x_i)$。损失函数就是用来计算 $x_i$ 真实值 $y_i$ 与预测值 $f(x_i)$ 之间差值的函数。

很显然，在训练过程中希望映射函数在训练数据集上得到“损失”之和最小，即：

$$
\min \sum_{i=1}^{n} \text{Loss}(f(x_i), y_i)
$$

| 损失函数名称              | 损失函数定义                                                 |
| ------------------------- | ------------------------------------------------------------ |
| 0-1 损失函数              | $$  \text{Loss}(y_i, f(x_i)) =  [f(x_i) = y_i]  $$ |
| 平方损失函数              | $$ \text{Loss}(y_i, f(x_i)) = (y_i - f(x_i))^2 $$            |
| 绝对损失函数              | $$ \text{Loss}(y_i, f(x_i)) = \|y_i - f(x_i)\|  $$              |
| 对数损失函数/对数似然损失 | $$ \text{Loss}(y_i, P(y_i\|x_i)) = -\log(P(y_i\|x_i)) $$       |

## 过学习和欠学习

| 经验风险（训练集上表现） | 期望风险（测试集上表现） | 描述                   |
| ------------------------ | ------------------------ | ---------------------- |
| 小                       | 小                       | 泛化能力强             |
| 小                       | 大                       | 过学习（模型过于复杂） |
| 大                       | 大                       | 欠学习                 |
| 大                       | 小                       | “神仙算法”或“黄梁美梦” |

- 结构风险最小化（structural risk minimization）：防止过学习，基于过学习时参数值通常都较大这一发现，在经验风险上加上表示模型复杂度的正则化项（regularizer）或惩罚项（penalty term），在最小化经验风险与降低模型复杂度之间寻找平衡

## 监督学习方法

### 判别方法

判别方法直接对**条件概率 $P(Y|X)$** 进行建模，即给定输入 $X$，预测输出 $Y$。

**优点**：

- 通常计算效率较高，因为直接学习分类边界。
- 在分类任务中表现通常更好，尤其是当训练数据充足时。

**缺点**：

  - 无法生成新的数据样本，因为不建模数据的分布。
  - 对数据的分布假设较少，可能对噪声敏感。

**常见算法**：回归模型、神经网络、支持向量机和 Ada boosting.

### 生成方法

生成方法对**联合概率 $P(X, Y)$** 进行建模，即同时学习输入 $X$ 和输出 $Y$ 的分布，生成方法的目标是理解数据的生成过程。

**优点**：

- 可以生成新的数据样本（如生成图像、文本，即可以搞出新的 $X$）。
- 对数据的分布有更全面的理解，适合小样本学习。

**缺点**：

- 计算复杂度较高，因为需要建模整个数据的分布。
- 联合分布概率 $P(X,Y)$ 或似然概率 $P(X|Y)$ 求取很困难。
- 在分类任务中，性能可能不如判别方法。

**常见算法：**贝叶斯方法、隐马尔可夫链。

## 线性回归

### 一元线性回归

$$
\begin{align*}
h_\theta(x) &= \theta_0 + \theta_1 x \\
J(\theta_0, \theta_1) &= \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2
\end{align*}
$$

#### 梯度下降法

即：

$$
\begin{align*}
\text{temp0} := \theta_{0} - \alpha \frac{\partial}{\partial \theta_{0}} J(\theta_{0}, \theta_{1}) \\
\text{temp1} := \theta_{1} - \alpha \frac{\partial}{\partial \theta_{1}} J(\theta_{0}, \theta_{1}) \\
\theta_{0} := \text{temp0} \\
\theta_{1} := \text{temp1}
\end{align*}
$$

其中 $\alpha$ 是学习率，控制步幅。 

#### 最小二乘法

回归模型：$y_i = ax_i + b \ (1 \leq i \leq n)$

通过最小化目标函数：

$$
L(a, b) = \sum_{i=1}^n (y_i - ax_i - b)^2
$$

对 $a$ 和 $b$ 求偏导并令其为 0，可得到：

$$
\frac{\partial L(a, b)}{\partial a} = \sum_{i=1}^n 2(y_i - ax_i - b)(-x_i) = 0
$$

$$
\frac{\partial L(a, b)}{\partial b} = \sum_{i=1}^n 2(y_i - ax_i - b)(-1) = 0
$$

将 $b = \bar{y} - a\bar{x}$（其中 $\bar{x} = \frac{\sum_{i=1}^n x_i}{n}$, $\bar{y} = \frac{\sum_{i=1}^n y_i}{n}$）代入整理，得到斜率 $a$ 的解：

$$
a = \frac{\sum_{i=1}^n x_i y_i - n\bar{x}\bar{y}}{\sum_{i=1}^n x_i x_i - n\bar{x}^2}
$$

截距 $b$ 可通过：

$$
b = \bar{y} - a\bar{x}
$$

### 多元线性回归

多元线性回归模型用于描述因变量 $y$ 与多个自变量 $x_1, x_2, \dots, x_n$ 之间的线性关系，其数学模型为：

$$
y = a_0 + a_1 x_1 + a_2 x_2 + \dots + a_n x_n
$$

可以写成矩阵形式：

$$
y = X^T a
$$

其中：

- $X = [x_1, x_2, \dots, x_m]$ 是包含所有自变量的设计矩阵（扩展后每行加一列常数项 1）；
- $a = [a_0, a_1, \dots, a_n]^T$ 是回归系数的列向量；
- $y = [y_1, y_2, \dots, y_m]^T$ 是目标变量的列向量。

#### 最小化均方误差

为找到最优的回归系数向量 $a$，我们使用均方误差 (MSE) 作为目标函数：

$$
J_m = \frac{1}{m} \sum_{i=1}^m (y_i - f(x_i))^2
$$

将其写成矩阵形式：

$$
J_m(a) = \frac{1}{m} (y - X^T a)^T (y - X^T a)
$$

#### 梯度计算与求解

通过对目标函数 $J_m(a)$ 求导并令其为 0，可以得到回归系数 $a$ 的解析解：

$$
\nabla J(a) = -2 X (y - X^T a)
$$

令 $\nabla J(a) = 0$，得到：

$$
X X^T a = X y
$$

求解上式可得到回归系数向量：

$$
a = (X X^T)^{-1} X y
$$

#### 结论

最终，多元线性回归模型的参数 $a$ 可以通过以上公式计算得到：

$$
a = (X X^T)^{-1} X y
$$

这是一种基于最小二乘法的解析解，适用于中小规模数据集，能够快速计算回归系数。

### 逻辑回归

逻辑回归是一种广泛用于分类任务的模型，适合二分类问题（如正类和负类）。虽然名字中带有“回归”，但其目标是预测一个样本**属于某一类的概率，而非连续值**。

逻辑回归模型假设目标变量 $y \in \{0, 1\}$，并通过如下模型定义：

$$
h_\theta(x) = \sigma(\theta^T x)
$$

其中：

- $h_\theta(x)$ 表示样本属于正类的概率；
- $\sigma(z)$ 是 **Sigmoid** 函数，定义为：

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

Sigmoid 函数将线性回归模型 $\theta^T x$ 的输出映射到 $(0, 1)$ 区间，便于解释为概率。

#### 损失函数

为了训练逻辑回归模型，最小化以下的对数损失函数：

$$
J(\theta) = -\frac{1}{m} \sum_{i=1}^m \big[ y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)}) ) \big]
$$

#### 优化方法

使用梯度下降法优化参数 $\theta$，更新规则为：

$$
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
$$

#### 应用场景

逻辑回归广泛用于金融风控（如用户是否违约）、医疗诊断（如疾病预测）以及各种分类问题中，具有简单、快速、可解释的特点。

## 决策树

### 什么是决策树？

决策树是一种树形结构的分类和回归方法，能够通过属性的分割将数据集划分为不同类别。其主要特征包括：

- **节点与分支**：
    - 每个非叶子节点表示对某个属性的判断。
    - 每个分支代表基于该属性值的决策。
    - 每个叶子节点代表分类或预测的最终结果。

- 决策树通过将复杂问题分解为多个简单的基于单个信息的推理任务，利用树状结构逐步完成决策过程。

### 决策树的相关系数

在构建决策树的过程中，我们需要选择最优的属性来分割数据。选择依据通常是属性的“纯度”。衡量纯度的两个重要概念是熵（Entropy）和信息增益（Information Gain）。

**熵**用来衡量样本集合的不确定性程度，熵越高，说明集合的不确定性越大，纯度越低。给定一个样本集合 $D$，其熵定义为：
$$
H(D) = - \sum_{k=1}^K p_k \log_2 p_k
$$

其中：

- $K$ 是类别的数量；
- $p_k$ 是样本属于第 $k$ 类的概率。

**信息增益**表示选择某个属性后，样本集合的不确定性减少的程度。某属性 $A$ 对样本集合 $D$ 的信息增益定义为：
$$
IG(D, A) = H(D) - \sum_{v \in \text{Values}(A)} \frac{|D_v|}{|D|} H(D_v)
$$

其中：

- $\text{Values}(A)$ 表示属性 $A$ 的可能取值；
- $D_v$ 是样本集合 $D$ 中属性 $A$ 取值为 $v$ 的子集。

信息增益越大，表示该属性对分类越有帮助。

**增益率**定义为信息增益与分裂信息的比值：
$$
\text{Gain Ratio}(D, A) = \frac{\text{Gain}(D, A)}{\text{info}(D)}
$$
但是存在的问题是如果我们以增益率为标准分裂，系统会偏向于选择信息较少的部分。

**基尼系数**是一种简单高效的纯度衡量指标，与熵相比，基尼系数无需计算对数，计算更为简单。
$$
\text{Gini}(D) = 1 - \sum_{k=1}^K p_k^2
$$
当样本全属于一个类别时，基尼系数为 0，表示纯度最高；当样本均匀分布于各类别时，基尼系数最大。

### 决策树的构建

构建决策树的过程主要包括以下步骤：

1. **选择最优属性**：
    - 计算所有属性的信息增益；
    - 选择信息增益 / 增益率 / 基尼系数最大的属性作为当前节点的分裂属性。

2. **分割样本集**：
    - 根据选择的分裂属性，将样本集划分为若干子集，每个子集对应一个分支。

3. **递归构建子树**：
    - 对每个子集重复上述过程，直到满足停止条件：
        - 子集纯度达到100%（大家都长一样）；
        - 或者样本集属性为空；
        - 或者样本数量不足以继续分裂。

### 连续属性离散化

有些时候，一些特征可能是连续的，我们需要运用离散化的方法，便于分类分支。

1. **属性排序**：

    - 假设连续属性 $a$ 在样本集 $D$ 中出现 $n$ 个不同的取值，按从小到大的顺序排列，记为 $a^1, a^2, \dots, a^n$。

2. **候选分割点**：

    - 基于分割点 $t$，可以将样本集 $D$ 分为两个子集：
        - $D_t^- = \{x \in D | a(x) \leq t\}$，即属性 $a$ 的值不大于分割点 $t$ 的样本集；
        - $D_t^+ = \{x \in D | a(x) > t\}$，即属性 $a$ 的值大于分割点 $t$ 的样本集。

3. **分割点集合**：

    - 考虑所有相邻取值的中位点作为候选分割点集合：

        $$
        T_a = \left\{ \frac{a^i + a^{i+1}}{2} \ \middle|\ 1 \leq i \leq n-1 \right\}
        $$

    - 这里，$\frac{a^i + a^{i+1}}{2}$ 是相邻取值 $a^i$ 和 $a^{i+1}$ 的中点。

4. **选取最优划分点：** 分割完毕以后，选取最优的划分点，对集合进行二分。

### 剪枝（Pruning）

为了防止过拟合，决策树模型通常会进行剪枝。剪枝分为两种方式：

1. **预剪枝**：
    - 在构建过程中提前停止分裂，避免生成过深的树。
    - 通过设定条件（如节点样本数、信息增益阈值）判断是否继续分裂。

2. **后剪枝**：
    - 在决策树生成后，评估子树对整体模型性能的影响，并选择性地合并节点。
