# 机器学习

## 基本概念

机器学习的目标是从原始数据中提取特征，学习一个映射函数 f 将上述特征（或原始数据）映射到语义空间，寻找数据和任务目标之间的关系。

**机器学习种类：**

- 监督学习
    - 给定带有标签信息的训练集合，学习从输入到输出的映射
    - 一般被应用在回归或分类的任务中
- 无监督学习
    - 最大特点是**数据无标签**
    - 一般被应用在聚类或若干降维任务中
    - 半监督学习依赖于部分被标注的数据
- 强化学习
    - 一种序列数据决策学习方法
    - 从与环境交互中学习，通过回报值（reward）让智能体（agent）学习到在不同状态（state）下如何选择行为方式（action）

## 监督学习

监督学习一般包含三个部分内容：

- 从训练数据集中学习得到映射函数 f
- 在测试数据集上测试映射函数 f
- 在未知数据集上测试映射函数 f（投入使用）

训练及中产生的损失一般称为经验风险（empirical risk），越小对训练集拟合效果越好；

测试集中加入从真实数据分布采样的样本时，测试集上的损失会不断逼近期望风险（expected risk），越小模型越好；

机器学习的目标是追求期望风险最小化。

### 损失函数

训练集中一共有 $n$ 个标注数据，第 $i$ 个标注数据记为 $(x_i, y_i)$，其中第 $i$ 个样本数据为 $x_i$，$y_i$ 是 $x_i$ 的标注信息。

从训练数据中学习得到的映射函数记为 $f$，$f$ 对 $x_i$ 的预测结果记为 $f(x_i)$。损失函数就是用来计算 $x_i$ 真实值 $y_i$ 与预测值 $f(x_i)$ 之间差值的函数。

很显然，在训练过程中希望映射函数在训练数据集上得到“损失”之和最小，即：

$$
\min \sum_{i=1}^{n} \text{Loss}(f(x_i), y_i)
$$

| 损失函数名称       | 损失函数定义                                                                 |
|--------------------|------------------------------------------------------------------------------|
| 0-1 损失函数       | $$ \text{Loss}(y_i, f(x_i)) = \begin{cases} 0, & f(x_i) = y_i \\ 1, & f(x_i) \neq y_i \end{cases} $$ |
| 平方损失函数       | $$ \text{Loss}(y_i, f(x_i)) = (y_i - f(x_i))^2 $$                            |
| 绝对损失函数       | $$ \text{Loss}(y_i, f(x_i)) = |y_i - f(x_i)| $$                              |
| 对数损失函数/对数似然损失 | $$ \text{Loss}(y_i, P(y_i|x_i)) = -\log(P(y_i|x_i)) $$                       |

### 过学习和欠学习

| 经验风险（训练集上表现） | 期望风险（测试集上表现） | 描述                   |
| ------------------------ | ------------------------ | ---------------------- |
| 小                       | 小                       | 泛化能力强             |
| 小                       | 大                       | 过学习（模型过于复杂） |
| 大                       | 大                       | 欠学习                 |
| 大                       | 小                       | “神仙算法”或“黄梁美梦” |

- 结构风险最小化（structural risk minimization）：防止过学习，基于过学习时参数值通常都较大这一发现，在经验风险上加上表示模型复杂度的正则化项（regularizer）或惩罚项（penalty term），在最小化经验风险与降低模型复杂度之间寻找平衡

### 监督学习方法

#### 判别方法

判别方法直接对**条件概率 $P(Y|X)$** 进行建模，即给定输入 $X$，预测输出 $Y$。

**优点**：

- 通常计算效率较高，因为直接学习分类边界。
- 在分类任务中表现通常更好，尤其是当训练数据充足时。

**缺点**：

  - 无法生成新的数据样本，因为不建模数据的分布。
  - 对数据的分布假设较少，可能对噪声敏感。

**常见算法**：回归模型、神经网络、支持向量机和 Ada boosting.

#### 生成方法

生成方法对**联合概率 $P(X, Y)$** 进行建模，即同时学习输入 $X$ 和输出 $Y$​ 的分布，生成方法的目标是理解数据的生成过程。

**优点**：

- 可以生成新的数据样本（如生成图像、文本，即可以搞出新的 $X$）。
- 对数据的分布有更全面的理解，适合小样本学习。

**缺点**：

- 计算复杂度较高，因为需要建模整个数据的分布。
- 联合分布概率 $P(X,Y)$ 或似然概率 $P(X|Y)$ 求取很困难。
- 在分类任务中，性能可能不如判别方法。

**常见算法：**贝叶斯方法、隐马尔可夫链。

### 线性回归



## 无监督学习